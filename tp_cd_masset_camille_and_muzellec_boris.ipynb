{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Proximal coordinate descent method on regression models\n",
    "\n",
    "#### Authors: S. Gaiffas, A. Gramfort\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- proximal coordinate descent\n",
    "\n",
    "for \n",
    "- Lasso / L1 linear regression\n",
    "- non-negative least squares (NNLS)\n",
    "\n",
    "models.\n",
    "\n",
    "The proximal operators we will use are the \n",
    "- L1 penalization\n",
    "- indicator function of $\\mathbb{R}_+$\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 23th of october at 23:59**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called **Rendu TP du 17 octobre 2016**. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"camille\"\n",
    "ln1 = \"masset\"\n",
    "fn2 = \"boris\"\n",
    "ln2 = \"muzellec\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp_cd\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## to embed figures in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 : Introduction\n",
    "\n",
    "We'll start by generating sparse positive vectors and simulating data\n",
    "\n",
    "### Getting sparse coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=2)  # to have simpler print outputs with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "n_samples = 1000\n",
    "idx = np.arange(n_features)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[20:] = 0.\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Parameters / Coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the simulation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu_linreg(coefs, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a linear regression model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : `numpy.array`, shape=(n_features,)\n",
    "        Coefficients of the model\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It samples of a centered Gaussian \n",
    "        vector with covariance given by the Toeplitz matrix\n",
    "    \n",
    "    b : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    # Construction of a covariance matrix\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    # Simulation of features\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    # Simulation of the labels\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal operators and Solver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind that the proximal operator of a fonction $g$ is given by:\n",
    "\n",
    "$$\n",
    "\\text{prox}_g(y, t) = \\arg\\min_x \\Big\\{ \\frac 12 \\|x - y\\|_2^2 + t g(x) \\Big\\}.\n",
    "$$\n",
    "\n",
    "where $t \\geq 0$ is a non-negative number.\n",
    "We have in mind to use the following cases\n",
    "\n",
    "- Lasso penalization, where $g(x) = s \\|x\\|_1$\n",
    "- Indicator function of $\\mathbb{R}_+$, where $g(x) = i_{x \\geq 0}(\\cdot)$\n",
    "\n",
    "where $s \\geq 0$ is a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize:\n",
    "$$\n",
    "\\arg\\min_x F(x)\n",
    "$$\n",
    "with\n",
    "$$\n",
    " F(x) = \\frac{1}{2} \\|b - Ax\\|^2 + g(x)\n",
    "$$\n",
    "\n",
    "## Questions\n",
    "\n",
    "- Code a function that computes $g(x)$ and $\\text{prox}_g(x)$ for in both cases\n",
    "- Justify why proximal coordinate descent can be applied to obtain a minimum of such objective functions.\n",
    "- Starting from the code provided in the notebook presented during the coordinate descent course as well as the code below, implement a proximal coordinate method for both penalties.\n",
    "- Evaluate qualitatively the convergence when varying the conditioning of the problem.\n",
    "- Bonus: Try to show that coordinate is much less affected by bad conditioning that proximal gradient descent.\n",
    "\n",
    "### You are expected to implement the smart residuals updates !\n",
    "\n",
    "### You are very welcome to reuse everything you did for TP1 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso penalisation and proximal operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prox_lasso(x, s, t=1.):\n",
    "    \"\"\"Proximal operator for the Lasso at x with strength t\"\"\"\n",
    "    return np.multiply(np.sign(x), np.maximum(np.absolute(x) - s*t, 0))\n",
    "    \n",
    "def lasso(x, s):\n",
    "    \"\"\"Value of the Lasso penalization at x with strength t\"\"\"\n",
    "    return s*np.linalg.norm(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator function penalisation and proximal operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indicator(x, s):\n",
    "    \"\"\"Value of the indicator function of R+ penalisation at x with strength s\"\"\"\n",
    "    if s == 0:\n",
    "        return 0\n",
    "    return np.inf if any(x < 0) else 0\n",
    "\n",
    "def prox_indicator(x, s, t=1.):\n",
    "    \"\"\"Proximal operator for indicator function of R+ at x with strength s\"\"\"\n",
    "    return x if t*s == 0 else np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the proximal coordinate descent, because all the conditions that we saw in class are gathered:\n",
    "* $f$ is convex and has a Lipschitz-continuous gradient;\n",
    "* $g$ is convex and separable (non-smooth in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal coordinate descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cd_linreg(x0, A, b, g, prox_g, s=0., n_iter=50,\n",
    "              x_true=coefs, verbose=True):\n",
    "    \"\"\"Proximal gradient descent algorithm\n",
    "\n",
    "    Minimize :\n",
    "    \n",
    "    1/2 ||bâˆ’Ax||^2 + s * g(x)\n",
    "    \n",
    "    with coodinate descent.\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    r = b - A.dot(x)\n",
    "    n_samples, n_features = A.shape\n",
    "    \n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "    errors.append(err)\n",
    "    # Current objective\n",
    "    obj = 0.5 * np.linalg.norm(b - A.dot(x))**2 + g(x, s)\n",
    "    objectives.append(obj)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Lauching Coordinate Descent solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    \n",
    "    L = np.array([np.dot(A[:, i], A[:, i]) for i in range(A.shape[1])]) # we pre-compute Ai.T * Ai = Lipschitz constants\n",
    "    \n",
    "    for k in range(n_iter + 1):\n",
    "        i = k % n_features # cyclic feature selection\n",
    "        \n",
    "        x_new[i] += A[:, i].T.dot(r) / L[i]\n",
    "        x_new[i] = prox_g(x_new[i], s, 1/L[i])\n",
    "        r -= (x_new[i] - x[i]) * A[:, i]\n",
    "        x[i] = x_new[i]\n",
    "        \n",
    "        obj = 0.5 * np.linalg.norm(b - A.dot(x))**2 + g(x, s)\n",
    "        err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "        errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if k % 10 == 0 and verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, objectives, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal gradient descent (for comparison purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ista(x0, A, b, g, prox_g,  s=0., n_iter=50,\n",
    "         x_true=coefs, verbose=True):\n",
    "    \"\"\"Proximal gradient descent algorithm\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    n_samples, n_features = A.shape\n",
    "    \n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "    errors.append(err)\n",
    "    # Current objective\n",
    "    obj = 0.5 * np.linalg.norm(b - A.dot(x))**2 + g(x, s)\n",
    "    objectives.append(obj)\n",
    "    \n",
    "    #Lipschitz constant\n",
    "    L = np.linalg.norm(A.T.dot(A), 2)\n",
    "    \n",
    "    if verbose:\n",
    "        print (\"Lauching ISTA solver...\")\n",
    "        print (' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter + 1):\n",
    "\n",
    "        x = prox_g(x - 1/L*A.T.dot(A.dot(x)-b), s, 1/L)\n",
    "        \n",
    "        obj = 0.5 * np.linalg.norm(b - A.dot(x))**2 + g(x, s)\n",
    "        err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "        errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if k % 10 == 0 and verbose:\n",
    "            print (' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, objectives, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (reasonably) well conditionned problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix number of iterations, and multiply it by the number of features in the case of coordinate descent for fair comparision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "n_samples = 1000\n",
    "idx = np.arange(n_features)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[20:] = 0.\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Parameters / Coefficients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A, b = simu_linreg(coefs, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_hat, obj, err = ista(np.zeros(n_features), A, b, lasso, prox_lasso, s = 1e-02, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(err)), err, label=\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(obj)), obj, label=\"Objective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal coordinate descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_hat, obj, err = cd_linreg(np.zeros(n_features), A, b, lasso, prox_lasso, s = 1e-02, n_iter=n_iter*n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(err)), err, label=\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(obj)), obj, label=\"Objective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A badly conditionned problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "n_samples = 1000\n",
    "idx = np.arange(n_features)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[20:] = 0.\n",
    "coefs[3]*=10\n",
    "coefs[9]*=10\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Parameters / Coefficients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A, b = simu_linreg(coefs, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_hat, obj, err = ista(np.zeros(n_features), A, b, lasso, prox_lasso, s = 1e-02, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(obj)), obj, label=\"Objective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal coordinate descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_hat, obj, err = cd_linreg(np.zeros(n_features), A, b, lasso, prox_lasso, s = 1e-02, n_iter=n_iter*n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(obj)), obj, label=\"Objective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "As we can see, in the case of a well-conditionned problem, both methods achieve roughly the same objective value in the same running time, which was predictable since in a well-conditionned problem coordinate descent does not fully take advantage of using coordinate-tailored Lipschitz constant.\n",
    "\n",
    "However, in our badly-conditionned problem, coordinate descent achieves an objective value which is half that of proximal gradient descent. This shows that coordinate descent is more suited that proximal gradient descent in the case of poor conditionning, as it uses a different Lipschitz constant for each feature.\n",
    "\n",
    "Further, in the case of coordinate descent we observe that nothing hapens when we iterate over those features that have no importance. In our second example, were only 2 features are significant, we see that an improvement is achieved only twice every 50 iterations, i.e. when we descend on the 2 significant features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
